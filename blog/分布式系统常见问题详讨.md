# 分布式系统的若干问题讨论

## 一、基础理论与模型

### **1. CAP定理与一致性模型**


分布式系统的设计需在一致性（Consistency）、可用性（Availability）和分区容忍性（Partition Tolerance）之间权衡，

**一致性（Consistency）** : 所有节点访问同一份最新的数据副本

**可用性（Availability）**: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）

**分区容错性（Partition Tolerance）** : 分布式系统出现网络分区的时候，仍然能够对外提供服务

典型选择包括：

**CP系统**：优先保证强一致性和分区容忍性，牺牲部分可用性（如选举期间拒绝请求）

**AP系统**：优先可用性和分区容忍性，接受最终一致性。

**CA系统：**传统单机数据库

### 2.PACELC理论：CAP的扩展与细化

CAP定理是Eric Brewer在2000年提出的猜想，虽然在2002年得到了证明，但距离现在已经有20多年，我们大可抱着怀疑的态度来审视它。事实上，Eric Brewer自己在2012年写过一篇文章对CAP定理做了一版更新的解释：CAP Twelve Years Later: How the "Rules" Have Changed，其中补充了对延迟的描述。在其经典的解释中，CAP 定理忽略了延迟，尽管在实践中，延迟和分区是密切相关的。

2010年Daniel J. Abadi提出的PACELC可以看作是CAP定理的一个扩展或是校准。它其实很简单，核心说了两件事：

1. 如果存在分区P，那么系统需要选择A或者C，跟CAP一样；
2. 否则选择延迟Latency或是C // 要么延迟低 && 无法保证强一致性，要么延迟高 && 保证强一致。

PACELC理论是对CAP的补充，引入了延迟（Latency）维度，更贴近实际工程需求。

### **3.BASE理论**

**基本可用（Basically Available）**

系统在不可预见的故障（如网络分区、节点宕机）时，允许**损失部分功能或性能**，但核心服务仍可用。例如：电商大促时，评论模块故障但交易流程正常（功能降级）**、**数据库主节点宕机时，从节点提供只读服务（性能降级）。

**软状态（Soft State）**

允许系统中的数据存在**中间状态**（如未完成同步的副本），这些状态可能暂时不一致，但不影响整体服务。

分布式缓存（如 Redis 集群）允许节点间异步同步数据。消息队列（如 Kafka）中消息的“未确认”状态。

**最终一致性（Eventual Consistency）**

系统不保证数据实时一致，但经过**一段时间的同步后，所有副本会达成一致状态**。

 

- **CAP** 是分布式系统的“底层约束”，定义基本权衡原则。
- **PACELC** 是 CAP 的升级，引入无分区时的延迟优化维度，更贴近实际系统设计需求。
- **BASE** 是 CAP 在 AP 场景下的工程实践指南，强调最终一致性。
  三者共同构成分布式系统设计的理论体系，指导开发者在不同场景下做出合理权衡。

## 二、分布式数据库

### **1. 基于Proxy的分布式数据库**

通过将传统单机数据库作为数据节点，通过数据路由层（Proxy）形成分片规则，将不同的请求发送给不同的数据库实例。Proxy层与[DB层](https://zhida.zhihu.com/search?content_id=195136832&content_type=Article&match_order=1&q=DB层&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDE3MTI0NDksInEiOiJEQuWxgiIsInpoaWRhX3NvdXJjZSI6ImVudGl0eSIsImNvbnRlbnRfaWQiOjE5NTEzNjgzMiwiY29udGVudF90eXBlIjoiQXJ0aWNsZSIsIm1hdGNoX29yZGVyIjoxLCJ6ZF90b2tlbiI6bnVsbH0.IUQhEGRtyLjvrokSPR_YlRhv1UyGaHI3IK7LNF21H3s&zhida_source=entity)以SQL进行交互，分布式事务通常在Proxy层进行发起处理，包括负载均衡、SQL优化等工作。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261439709.png" alt="image-20250826143939626" style="zoom: 33%;" />

#### 水平分表

水平切分（分表）：是按照某种规则，将一个表的数据分散到多个物理独立的数据库服务器中，形成“独立”的数据库“分片”。多个分片共同组成一个逻辑完整的数据库实例。

常规的单机数据库中，一张完整的表仅在一个物理存储设备上读写。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261440810.png" alt="image-20250826144025715" style="zoom: 25%;" />

分布式数据库中，根据在建表时设定的分表键，系统将根据不同分表键自动分布数据到不同的物理分片中，但逻辑上仍然是一张完整的表。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261441014.png" alt="image-20250826144102978" style="zoom: 25%;" />

在 TDSQL MySQL版 中，数据的切分通常就需要找到一个分表键（shardkey）以确定拆分维度，再采用某个字段求模（HASH）的方案进行分表，而计算 HASH 的某个字段就是 shardkey。 HASH 算法能够基本保证数据相对均匀地分散在不同的物理设备中。

#### 数据读写

写入数据（ SQL 语句含有 shardkey ）

1. 业务写入一行数据。
2. 网关对 shardkey 进行 hash，得出 shardkey 的 hash 值。
3. 不同的 hash 值范围对应不同的分片（调度系统预先分片的算法决定）。
4. 数据根据分片算法，将数据存入实际对应的分片中。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261441634.png" alt="image-20250826144127543" style="zoom: 33%;" />

读取数据（有明确 shardkey 值）

1. 业务发送 select 请求中含有 shardkey 时，网关通过对 shardkey 进行 hash。
2. 不同的 hash 值范围对应不同的分片。
3. 数据根据分片算法，将数据从对应的分片中取出。

读取数据（无明确 shardkey 值）

1. 业务发送 select 请求没有 shardkey 时，将请求发往所有分片。
2. 各个分片查询自身内容，发回 Proxy 。
3. Proxy 根据 SQL 规则，对数据进行聚合，再答复给网关。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261441536.png" alt="image-20250826144158444" style="zoom:25%;" />

#### 扩容过程

TDSQL MySQL版 主要是采用自研的自动再均衡技术保证自动化的扩容和稳定。 

对需要扩容的 A 节点进行扩容操作。

1. 根据新加 G 节点配置，将 A 节点部分数据搬迁（从备机）到 G 节点。
2. 数据完全同步后，A、G 节点校验数据库，存在一至几十秒的只读，但整个服务不会停止。
3. 调度通知 proxy 切换路由。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261442060.png" alt="image-20250826144231963" style="zoom: 50%;" />

**如何保证扩容不影响到所有分片？**

理论上分片A-F的数据是hash(key) % 6 来进行数据分配，若新增分片hash(key) % 7会引起大量数据变更。

为了规避这一问题，此处hash使用**一致性hash**。

**缺点：**

无法做到无缝主备切换，可用性仍有提升的余地。

### **2. 原生分布式数据库**

具有无中心、扩展性好等特性，对业务感知较低或 无感知，与数据节点通过报文交互， 部分产品分布式事务可直接通过收到请求的数据节点进行协调，从而达到去中心化的能力，在扩展性方面具有一定的优势。

TDStore 实例分为集群版和基础版两种：

**集群版**：由多个（≥3个）节点构成，以三副本 Raft 集群的形态提供高性能可用的数据库服务，适用于企业生产环境。

**基础版**：由单个节点构成，以较低的成本提供完整的数据库功能，适用于个人用户。

#### TDStore 技术架构

无论是集中式单机数据库还是分布式数据库，功能模块通常可以分为三大组件：

计算引擎：主要包括 SQL 解析、优化器、执行器等。

存储引擎：主要包括事务处理、数据存储等。

元数据服务：主要包括全局逻辑时钟服务、全局 ID 生成器、元数据存储、调度引擎（数据/容灾调度）以及负载采集等。

TDStore 的总体架构如图所示：

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261442159.png" alt="image-20250826144245061" style="zoom:50%;" />

#### 计算引擎-SQLEngine

内核：基于 MySQL 8.0 实现，对 MySQL 兼容度高。

架构：计算层为多主架构，无状态化设计，每个 SQLEngine 节点均可读写，可以承受千万级别 QPS 写入压力。

交互：从管控节点获取全局事务时间戳和数据路由信息，然后与存储节点进行事务的交互，向客户端返回结果。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261443331.png" alt="image-20250826144311243" style="zoom:50%;" />

#### 存储引擎-TDStore

架构：基于 LSM-Tree 和 Multi-Raft 的分布式 KV 存储引擎。

数据：基于 Raft 同步的多副本的存储，数据根据 Key 范围分布在不同 Region 上。

交互：TDStore 接收来自计算节点的请求，处理后返回结果；每个 Region 的主副本负责接收和处理读写请求。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261443510.png" alt="image-20250826144325416" style="zoom:50%;" />

#### 存储模型

TDStore 的存储引擎会将所有用户数据映射到（-∞，+∞）的线性有序无限的 Key 空间中，每行数据对应到 key 空间中的某个点。

```
//示例create table t1 (  
f1 varchar(50),  
f2 varchar(20),  
f3 varchar(20),  
f4 varchar(20),  
primary key(f1,f2),  
index idx_f3(f3));
insert into t1 values('a', 'b', 'c', 'd');
```

在上图的示例中，insert 一行数据后，会产生两个 KV 键值对：

主键：pk_encode('t1', 'a', 'b') -> pk_value_encode('c', 'd')

索引：sk_encode('idx_f3', 'c', 'a', 'b') -> sk_value_encode()

因为每个 key 的最前面部分是该数据对象（表/索引/分区表）的 ID，故该数据对象的所有数据会连续分布在 Key 空间的某一段上。

三层组织模式（DO-Region-RG）

**数据对象（Database Object）**：例如表（table），分区（Partition），索引（Index），通常会为这些数据对象分配一个全局唯一的 ID（index_id）。

**数据分片（Region）**：

①：一段连续的，左闭右开的 key 空间，[startkey, endkey)。

②：一个数据库对象对应一个或多个 Region，且在运行过程中，可以根据数据调度策略，对 Region 进行分裂，或将多个 Region 合并为一个 Region。

③：数据调度的基本单位，不同的 region 可以分布在该实例中的任何节点。

④：Region 是一个逻辑概念，在底层存储上，并不是完全独立的一段或一个文件。

**复制组（Replication Group）**：

①：一个复制组中可以包含一个或多个 Region，这些 region 属于一个或多个数据对象。

②：在运行过程中，可以合并 RG，也可以分裂 RG（具体实现中通过创建 RG，迁移 region 进入或离开 RG 等来实现）。

③：一个 RG，对应一个 Raft 日志流（Redolog/WAL），如果一个事务（transaction）所涉及到的 key 都在一个 RG 内，则该事务可转换为单机事务，如果一个事务涉及数据跨 RG，则需要走两阶段的分布式事务协议。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261444511.png" alt="image-20250826144402411" style="zoom:50%;" />

 

## 三、容灾

金融行业容灾架构演进

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261444405.png" alt="image-20250826144447366" style="zoom:33%;" />

### **1. 冷备**

为了解决应用及数据单点问题，最初引入了冷备技术。这里谈到的冷备包含多层含义。原始的需求，就是将数据放在异地进行备份。当主环境出现问题时，可利用异地备份还原数据即可。整个业务恢复时间取决于数据恢复时间及重新部署应用的时间。为了解决应用部署问题，后续将应用在异地进行部署，但不启动，这样解决了应用部署问题。为解决恢复数据时间问题，可通过数据库的主从复制技术，提升数据完整度及故障恢复能力。无论上述如何改进，本质上来说都是冷备技术，备的部分不提供访问能力，存在一定的资源浪费。

### **2. 主备**

为解决上面资源浪费问题，出现了主备技术。其基本结构上与冷备差异不大，更多是在备角色的使用上面。通过将部分应用部署到备，充分利用备的资源。但受到[数据同步](https://cloud.tencent.com/product/datainlong?from_column=20065&from=20065)限制，无法做到完美一致性，同时是考虑将备充当读取业务的承载，来分担主业务压力的同时，利用备的资源。为了充分利用备的应用能力，可以启用备的写应用，但是数据访问上仍然是访问主的数据。这种方式可以加快故障后的切换速度，同时也能启动一定的验证作用；但这种架构也有一个问题，就在于应对较大范围故障问题存在不足，当发生如机房级故障时无法做到切换。

### **3. 多活（双活）**

主备架构容灾能力有限，也促生了多活架构。所谓多活架构，简单来说是应用系统与基础架构配合，通过将业务处理单元化实现更大范围的容灾能力。根据实现方式可分为同城双活和异地多活两种方式。这部分是本文的重点，后面会详细说明。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261445406.png" alt="image-20250826144511322" style="zoom:50%;" />



**《关于银行业保险业数字化转型的指导意见》（银保监办发〔2022〕2号）**

- **核心要求**：
  - **数据中心高可用性**：核心业务系统需实现 **同城双活+异地多活**，RPO≤0（零数据丢失），RTO≤5分钟。
- **灾备能力分级**：

| 系统级别         | RPO要求           | RTO要求           | 容灾等级             |
| :--------------- | :---------------- | :---------------- | :------------------- |
| **核心支付系统** | 0（强一致性同步） | ≤30秒（自动切换） | 6级（多活架构）      |
| **重要业务系统** | ≤5秒（准实时）    | ≤5分钟            | 5级（热备+自动切换） |
| **一般业务系统** | ≤5分钟            | ≤2小时            | 4级（异步复制）      |

## 四、关键机制

### 1. 存储引擎

**日志结构（log-structured）** 的存储引擎，以及 **面向页面（page-oriented）** 的存储引擎（例如 B 树）

#### LSM树

最简单的数据库可以用两个 Bash 函数实现：

```
#!/bin/bash
db_set () {
  echo "$1,$2" >> database
}

db_get () {
  grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}
```

这两个函数实现了键值存储的功能。执行 `db_set key value` 会将 **键（key）** 和 **值（value）** 存储在数据库中。键和值（几乎）可以是你喜欢的任何东西，例如，值可以是 JSON 文档。然后调用 `db_get key` 会查找与该键关联的最新值并将其返回。

麻雀虽小，五脏俱全：

```
$ db_set 123456 '{"name":"London","attractions":["Big Ben","London Eye"]}'

$ db_set 42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'

$ db_get 42
{"name":"San Francisco","attractions":["Golden Gate Bridge"]}
```

底层的存储格式非常简单：一个文本文件，每行包含一条逗号分隔的键值对（忽略转义问题的话，大致与 CSV 文件类似）。每次对 `db_set` 的调用都会向文件末尾追加记录，所以更新键的时候旧版本的值不会被覆盖 —— 因而查找最新值的时候，需要找到文件中键最后一次出现的位置（因此 `db_get` 中使用了 `tail -n 1` )。

```
$ db_set 42 '{"name":"San Francisco","attractions":["Exploratorium"]}'

$ db_get 42
{"name":"San Francisco","attractions":["Exploratorium"]}

$ cat database
123456,{"name":"London","attractions":["Big Ben","London Eye"]}
42,{"name":"San Francisco","attractions":["Golden Gate Bridge"]}
42,{"name":"San Francisco","attractions":["Exploratorium"]}
```

`db_set` 函数对于极其简单的场景其实有非常好的性能，因为在文件尾部追加写入通常是非常高效的。与 `db_set` 做的事情类似，许多数据库在内部使用了 **日志（log）**，也就是一个 **仅追加（append-only）** 的数据文件。真正的数据库有更多的问题需要处理（如并发控制，回收硬盘空间以避免日志无限增长，处理错误与部分写入的记录），但基本原理是一样的。日志极其有用，我们还将在本书的其它部分重复见到它好几次。

**键值更新日志，只保留每个键的最近值**

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261445887.png" alt="image-20250826144528793" style="zoom:50%;" />

**同时执行压缩和分段合并**

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261445168.png" alt="image-20250826144548069" style="zoom:50%;" />

每个日志结构存储段都是一系列键值对。这些键值对按照它们写入的顺序排列，日志中稍后的值优先于日志中较早的相同键的值。除此之外，文件中键值对的顺序并不重要。

现在我们可以对段文件的格式做一个简单的改变：要求键值对的序列按键排序。乍一看，这个要求似乎打破了我们使用顺序写入的能力，我们将稍后再回到这个问题。

我们把这个格式称为 **排序字符串表（Sorted String Table）**，简称 SSTable。我们还要求每个键只在每个合并的段文件中出现一次（压缩过程已经保证）。与使用散列索引的日志段相比，SSTable 有几个大的优势：

1. 即使文件大于可用内存，合并段的操作仍然是简单而高效的。这种方法就像归并排序算法中使用的方法一样，你开始并排读取多个输入文件，查看每个文件中的第一个键，复制最低的键（根据排序顺序）到输出文件，不断重复此步骤，将产生一个新的合并段文件，而且它也是也按键排序的。**合并几个 SSTable 段，只保留每个键的最新值**

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261446812.png" alt="image-20250826144605716" style="zoom: 33%;" />

**具有内存索引的 SSTable**



<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261446762.png" alt="image-20250826144617669" style="zoom:33%;" />



到目前为止还不错，但是如何让你的数据能够预先排好序呢？毕竟我们接收到的写入请求可能以任何顺序发生。

现在我们可以让我们的存储引擎以如下方式工作：

- 有新写入时，将其添加到内存中的平衡树数据结构（例如红黑树）。这个内存树有时被称为 **内存表（memtable）**。
- 当 **内存表** 大于某个阈值（通常为几兆字节）时，将其作为 SSTable 文件写入硬盘。这可以高效地完成，因为树已经维护了按键排序的键值对。新的 SSTable 文件将成为数据库中最新的段。当该 SSTable 被写入硬盘时，新的写入可以在一个新的内存表实例上继续进行。
- 收到读取请求时，首先尝试在内存表中找到对应的键，如果没有就在最近的硬盘段中寻找，如果还没有就在下一个较旧的段中继续寻找，以此类推。
- 时不时地，在后台运行一个合并和压缩过程，以合并段文件并将已覆盖或已删除的值丢弃掉。

这个方案效果很好。它只会遇到一个问题：如果数据库崩溃，则最近的写入（在内存表中，但尚未写入硬盘）将丢失。为了避免这个问题，我们可以在硬盘上保存一个单独的日志，每个写入都会立即被追加到这个日志上，就像在前面的章节中所描述的那样。这个日志没有按排序顺序，但这并不重要，因为它的唯一目的是在崩溃后恢复内存表。每当内存表写出到 SSTable 时，相应的日志都可以被丢弃。

**性能优化**

与往常一样，要让存储引擎在实践中表现良好涉及到大量设计细节。例如，当查找数据库中不存在的键时，LSM 树算法可能会很慢：你必须先检查内存表，然后查看从最近的到最旧的所有的段（可能还必须从硬盘读取每一个段文件），然后才能确定这个键不存在。为了优化这种访问，存储引擎通常使用额外的**布隆过滤器**（布隆过滤器是一种节省内存的数据结构，用于近似表达集合的内容，它可以告诉你数据库中是否存在某个键，从而为不存在的键节省掉许多不必要的硬盘读取操作。）

还有一些不同的策略来确定 SSTables 被压缩和合并的顺序和时间。最常见的选择是 size-tiered 和 leveled compaction。LevelDB 和 RocksDB 使用 leveled compaction（LevelDB 因此得名），HBase 使用 size-tiered。对于 sized-tiered，较新和较小的 SSTables 相继被合并到较旧的和较大的 SSTable 中。对于 leveled compaction，key （按照分布范围）被拆分到较小的 SSTables，而较旧的数据被移动到单独的层级（level），这使得压缩（compaction）能够更加增量地进行，并且使用较少的硬盘空间。

即使有许多微妙的东西，LSM 树的基本思想 —— 保存一系列在后台合并的 SSTables —— 简单而有效。即使数据集比可用内存大得多，它仍能继续正常工作。由于数据按排序顺序存储，你可以高效地执行范围查询（扫描所有从某个最小值到某个最大值之间的所有键），并且因为硬盘写入是连续的，所以 LSM 树可以支持非常高的写入吞吐量。

#### B树

前面讨论的日志结构索引看起来已经相当可用了，但它们却不是最常见的索引类型。使用最广泛的索引结构和日志结构索引相当不同，它就是我们接下来要讨论的 B 树。

从 1970 年被引入，仅不到 10 年后就变得 “无处不在”，B 树很好地经受了时间的考验。在几乎所有的关系数据库中，它们仍然是标准的索引实现，许多非关系数据库也会使用到 B 树。

像 SSTables 一样，B 树保持按键排序的键值对，这允许高效的键值查找和范围查询。但这也就是仅有的相似之处了：B 树有着非常不同的设计理念。

我们前面看到的日志结构索引将数据库分解为可变大小的段，通常是几兆字节或更大的大小，并且总是按顺序写入段。相比之下，B 树将数据库分解成固定大小的 **块（block）** 或 **分页（page）**，传统上大小为 4KB（有时会更大），并且一次只能读取或写入一个页面。这种设计更接近于底层硬件，因为硬盘空间也是按固定大小的块来组织的。

每个页面都可以使用地址或位置来标识，这允许一个页面引用另一个页面 —— 类似于指针，但在硬盘而不是在内存中。我们可以使用这些页面引用来构建一个页面树.

**使用 B 树索引查找一个键**

一个页面会被指定为 B 树的根；在索引中查找一个键时，就从这里开始。该页面包含几个键和对子页面的引用。每个子页面负责一段连续范围的键，根页面上每两个引用之间的键，表示相邻子页面管理的键的范围（边界）。

在上图中，我们正在寻找键 251 ，所以我们知道我们需要跟踪边界 200 和 300 之间的页面引用。这将我们带到一个类似的页面，进一步将 200 到 300 的范围拆分到子范围。

最终，我们将到达某个包含单个键的页面（叶子页面，leaf page），该页面或者直接包含每个键的值，或者包含了对可以找到值的页面的引用。

如果要更新 B 树中现有键的值，需要搜索包含该键的叶子页面，更改该页面中的值，并将该页面写回到硬盘（对该页面的任何引用都将保持有效）。如果你想添加一个新的键，你需要找到其范围能包含新键的页面，并将其添加到该页面。**如果页面中没有足够的可用空间容纳新键，则将其分成两个半满页面，并更新父页面以反映新的键范围分区。**

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261449838.png" alt="image-20250826144910787" style="zoom: 33%;" />

#### 比较B树和LSM树

尽管 B 树实现通常比 LSM 树实现更成熟，LSM 树由于其性能特征的关系，仍然引起了不少关注。根据经验，通常 LSM 树的写入速度更快，而 B 树的读取速度更快。LSM 树上的读取通常比较慢，因为它们必须检查几种不同的数据结构和不同压缩（Compaction）层级的 SSTables。

然而，基准测试的结果通常和工作负载的细节相关。你需要用你特有的工作负载来测试系统，以便进行有效的比较。在本节中，我们将简要讨论一些在衡量存储引擎性能时值得考虑的事情。

**LSM树的优点**

B 树索引中的每块数据都必须至少写入两次：一次写入预先写入日志（WAL），一次写入树页面本身（如果有分页还需要再写入一次）。即使在该页面中只有几个字节发生了变化，也需要接受写入整个页面的开销。有些存储引擎甚至会覆写同一个页面两次，以免在电源故障的情况下页面未完整更新。

由于反复压缩和合并 SSTables，日志结构索引也会多次重写数据。这种影响 —— 在数据库的生命周期中每笔数据导致对硬盘的多次写入 —— 被称为 **写入放大（write amplification）**。使用固态硬盘的机器需要额外关注这点，固态硬盘的闪存寿命在覆写有限次数后就会耗尽。

在写入繁重的应用程序中，性能瓶颈可能是数据库可以写入硬盘的速度。在这种情况下，写放大会导致直接的性能代价：存储引擎写入硬盘的次数越多，可用硬盘带宽内它能处理的每秒写入次数就越少。

进而，LSM 树通常能够比 B 树支持更高的写入吞吐量，部分原因是它们有时具有较低的写放大（尽管这取决于存储引擎的配置和工作负载），部分是因为它们顺序地写入紧凑的 SSTable 文件而不是必须覆写树中的几个页面。这种差异在机械硬盘上尤其重要，其顺序写入比随机写入要快得多。

LSM 树可以被压缩得更好，因此通常能比 B 树在硬盘上产生更小的文件。B 树存储引擎会由于碎片化（fragmentation）而留下一些未使用的硬盘空间：当页面被拆分或某行不能放入现有页面时，页面中的某些空间仍未被使用。由于 LSM 树不是面向页面的，并且会通过定期重写 SSTables 以去除碎片，所以它们具有较低的存储开销，特别是当使用分层压缩（leveled compaction）时。

在许多固态硬盘上，固件内部使用了日志结构化算法，以将随机写入转变为顺序写入底层存储芯片，因此存储引擎写入模式的影响不太明显。但是，较低的写入放大率和减少的碎片仍然对固态硬盘更有利：更紧凑地表示数据允许在可用的 I/O 带宽内处理更多的读取和写入请求。

**LSM树的缺点**

日志结构存储的缺点是压缩过程有时会干扰正在进行的读写操作。尽管存储引擎尝试增量地执行压缩以尽量不影响并发访问，但是硬盘资源有限，所以很容易发生某个请求需要等待硬盘先完成昂贵的压缩操作。对吞吐量和平均响应时间的影响通常很小，但是日志结构化存储引擎在更高百分位的响应时间有时会相当长，而 B 树的行为则相对更具有可预测性。

压缩的另一个问题出现在高写入吞吐量时：硬盘的有限写入带宽需要在初始写入（记录日志和刷新内存表到硬盘）和在后台运行的压缩线程之间共享。写入空数据库时，可以使用全硬盘带宽进行初始写入，但数据库越大，压缩所需的硬盘带宽就越多。

如果写入吞吐量很高，并且压缩没有仔细配置好，有可能导致压缩跟不上写入速率。在这种情况下，硬盘上未合并段的数量不断增加，直到硬盘空间用完，读取速度也会减慢，因为它们需要检查更多的段文件。通常情况下，即使压缩无法跟上，基于 SSTable 的存储引擎也不会限制传入写入的速率，所以你需要进行明确的监控来检测这种情况【29,30】。

B 树的一个优点是每个键只存在于索引中的一个位置，而日志结构化的存储引擎可能在不同的段中有相同键的多个副本。这个方面使得 B 树在想要提供强大的事务语义的数据库中很有吸引力：在许多关系数据库中，事务隔离是通过在键范围上使用锁来实现的，在 B 树索引中，这些锁可以直接附加到树上。

B 树在数据库架构中是非常根深蒂固的，为许多工作负载都提供了始终如一的良好性能，所以它们不可能在短期内消失。在新的数据库中，日志结构化索引变得越来越流行。没有简单易行的办法来判断哪种类型的存储引擎对你的使用场景更好，所以需要通过一些测试来得到相关经验。

| 存储引擎 | B-Tree                                             | LSM-Tree                                                     | 备注                                                         |
| :------- | :------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| 优势     | 读取更快                                           | 写入更快                                                     |                                                              |
| 写放大   | 数据和 WAL更改数据时多次覆盖整个 Page              | 数据和 WALCompaction                                         | SSD 不能过多擦除。因此 SSD 内部的固件中也多用日志结构来减少随机小写。 |
| 写吞吐   | 相对较低：大量随机写。                             | 相对较高：较低的写放大（取决于数据和配置）顺序写入。更为紧凑。 |                                                              |
| 压缩率   | 存在较多内部碎片。                                 | 更加紧凑，没有内部碎片。压缩潜力更大（共享前缀）。           | 但紧缩不及时会造成 LSM-Tree 存在很多垃圾                     |
| 后台流量 | 更稳定可预测，不会受后台 compaction 突发流量影响。 | 写吞吐过高，compaction 跟不上，会进一步加重读放大。由于外存总带宽有限，compaction 会影响读写吞吐。随着数据越来越多，compaction 对正常写影响越来越大。 | RocksDB 写入太过快会引起 write stall，即限制写入，以期尽快 compaction 将数据下沉。 |
| 存储放大 | 有些 Page 没有用满                                 | 同一个 Key 存多遍                                            |                                                              |

### 2. 一致性哈希

hash（服务器的IP地址） % `2`^`32`

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261449944.png" alt="image-20250826144933852" style="zoom: 33%;" />

根据用户的 IP 使用上面相同的函数 Hash 计算出哈希值，并确定此数据在环上的位置，从此位置沿环 **顺时针行走**，遇到的第一台服务器就是其应该定位到的服务器。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261449970.png" alt="image-20250826144954872" style="zoom:33%;" />

Hash环的数据倾斜问题一致性哈希虽然为我们提供了稳定的切换策略，但是它也有一些小缺陷。因为 hash取模算法得到的结果是随机的，我们并不能保证各个服务节点能均匀的分配到哈希环上。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261450711.png" alt="image-20250826145015612" style="zoom:33%;" />



使用虚拟几点解决这个问题

`hash`(`"192.168.32.132#A"`) % 2^32 

`hash`(`"192.168.32.132#B"`) % 2^32

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261450756.png" alt="image-20250826145039661" style="zoom:33%;" />

### 3. raft算法

Paxos和Raft算法都是分布式一致性算法，它们的目的都是在一个分布式系统中保证数据的一致性。**这两个都是都是强一致性算法，因此也牺牲了可用性，CA系统。**

**可用性定义：非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。**

在一个分布式系统中，由于各个节点之间的网络延迟、节点故障等原因，数据同步可能会出现问题，这时候就需要使用一致性算法来保证数据的一致性。

Paxos算法是由Leslie Lamport在1998年提出的，它是一种经典的分布式一致性算法。Paxos算法使用的是一个基于消息传递的算法，它通过在不同的阶段进行消息传递来达成一致性。Paxos算法的基本流程包括：提议者（Proposer）向多个接受者（Acceptor）发起提议，接受者对提议进行投票，并将投票结果告知提议者，最终提议者根据接受者的投票结果确定一个值。

Raft算法是由Diego Ongaro和John Ousterhout在2013年提出的，它是一种相对较新的分布式一致性算法。Raft算法是一种更易于理解和实现的算法，它的基本思路是将分布式系统的状态机复制到多个节点上，然后使用选举机制来选出一个主节点来处理所有的客户端请求。在Raft算法中，主节点负责接收客户端请求，并将请求复制到其他节点上，从而保证了数据的一致性。

大名鼎鼎的容器编排系统Kubernetes 就使用了etcd作为底层存储。Google的 Spanner和微信使用的 PaxosStore都是基于 Paxos协议实现的 KV存储，早期 Paxos也是一致性协议的唯一标准，不过近年来 Raft协议热度主键上升，在 Google Trends中 2024 年的 Raft algorithm的热度已经是Paxos algorithm的2～3 倍，近年来的开源数据库包括 etcd、TiDB、CockroachDB也都基于 Raft协议。

 

推演一下raft算法过程。首先制定目标，然后逐渐完善机制达成目标。

1. 不超过一半的节点挂掉，不影响系统正常工作；
2. 强一致性，写入立马读到最新值；

 

为了方便介绍，以一个具体的场景说明。

假设中国银行共有北京、上海、南京、武汉、广州5个网点，用于办理存取款业务。同时每个网点有一台自己的**数据库**并配有一位**业务员**。数据库是个老古董，需要业务员手动数据录入，并且这个数据库反映迟钝，有时候录入数据点确定后要执行好久，但是好在这台服务器大多数时候靠谱，虽然效率慢但是总会执行成功。

这种情况下老板担心万一数据库坏了咋办，需要有备份。为此银行有规定，每个用户办理存取款的时候，除了录入当地数据库外，还要通知其他业务员去写数据库。因此为每位业务员配备一部**电话**，有录入任务时，业务员需要自己录入的同时分别给其他四个地区业务员打电话，其他四个地区业务员收到后，也要将这个信息写到数据库中。

业务办理高峰期的时候，每位业务员又要办理存取款业务，还要给其他业务员同步自己窗口的存取款记录，另外就是还会接到其他业务员打来的电话，忙得连轴转。而且有个问题，百忙之余，业务员还要把最近接听电话的内容和自己柜台统计的内容录入数据库，难免会遗漏和冲突。

为此五位员工商议做出来一个减少工作量的决定，每天只开放一个地区的柜台，所有人都只能到这一个柜台办业务。这样虽然这个柜台业务员累了，但老板同意给更多值班费，而其他四个人因为不用跟客户打交道轻松不少，五个人都开心了。开放柜台业务员我们称为**班长。**

班长负责柜台办理业务，所有业务员包括班长都配备一个**笔记本**，笔记本有统一的页码，并且每页只能写入一个事件。有事件发生时，班长会先将事件记录在自己笔记本下一页上，然后电话通知各支行业务员在同样页码写什么内容，业务员对是否成功写入做出回复。班长发现包括自己在内的**三个人对某一页的内容记好了，就电话周知将这一页的数据写数据库**。

**为何有三个业务员确认就可以落库？**

我们的设计目标是任意两个服务器损毁的情况下仍能有数据库存储完整数据。以“张三存3”这条数据为例，“张三存3”的信息落库到s1、s2、s3三个库中，随后最差的情况s1、s2损毁，仍然有s3存有该记录。

蓝色表示已落库。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261451751.png" alt="image-20250826145104657" style="zoom: 33%;" />

班长的工作包括：

1. 在柜台提供服务，可能随时会有客户来办理存取款或是查询余额。
2. 笔记本每页只能记录一个业务，办理每个业务，都会在笔记本下一页n记录某位用户的操作记录d，尝试打电话给所有业务员，存储n:d
3. 收集业务员的电话反馈，对第n页的记录，如果收到包括自己在内的3个人确认了，就认为这个内容落库。会在下次电话的时候说明n这一页落数据库。

业务员的工作包括：

1. 接听班长存储n:d组合的电话，写笔记本并回复班长是否写入正常。
2. 收到班长说n页确认落库的电话，将n页内容写到数据库中。

然而好久不长，又发现一种新的异常。我们假设业务员1是班长，他连续为张三和李四办理了两个业务。而办理张三业务的时候他没打通4、5电话，但是2、3电话都打通，并且2、3都回复确认并写笔记本了。所以班长周知第一页“张三存3”的信息落库，随后落到s1、s2、s3三个库中。然后以同样的方式“李四取5”的事件，被落库到了s1、s4、s5。紧接着，恰好服务器s1挂了，这是发现**没有任何一台服务器能同时保存所有历史记录**，这是不允许的。

因此设立新规定：**不能跳过空白页写笔记本，同时要保证各分行已经写入的内容，每一页内容都完全一致。**

为此设计了新的策略：

**电话时沟通时，班长除了告知当前页码和内容外，还要额外告知前一页的内容。业务员校验前一页是否有内容且相同，若相同则写入当前页内容，否则回复一个失败。**

之所以校验前一页内容，是因为不好一次性检查各分行之前已写入的每一页是否都一致，但是只要每次写新的一页时保证上一页一致，自然就**递推**得到保证。

班长如果接到明确的失败，他就知道这个分行的上一页是缺失或是错误的，下次电话同步时，他会给上一页和上上页的内容，以此类推进行探测和补全直到追赶上当前进度。

图中蓝色表示已落库，黄色表示正在同步，绿色表示收到同步写到笔记本但是还没落库，红色表示失败。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261457202.png" alt="image-20250826145731092" style="zoom:33%;" />

这样就解决了空白页问题，就算班长所在服务器挂了，其他地区服务器仍然至少有一台服务器存有所有历史数据，只要想办法将这个人重新**选举**为班长，新班长就可以带领大家把各自数据重建对齐到最新状态。

这里引入一个新的概念**任期，**因为班长可能会换，我们定义任期全局唯一并且每一任班长上任，任期都要+1，并在每次电话中都告诉业务员自己的任期是多少，业务员也会将任期写入笔记本的内容中。同时业务员也会单独记住当前任期，方便身份校验和下一任班长**选举**。此时笔记本一页存储的内容范例：

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261457088.png" alt="image-20250826145740003" style="zoom: 33%;" />

同时为了简化事件校验，所以我们以后表示一个事件只要用S（N，T）这种形式就可以了，页码N和任期T都相等，事件一定“相同”。

另外班长把已经确认的**最大确认落库页码加在了传给业务员的消息里**，这样业务员也能**结合自己的情况确定自己可以落库的最大页码**了。为了方便落库，还规定了**每个业务员把已落库最大页码以及可以安全落库最大页码分别记录一下**。

**选举问题**

班长或业务员可能出现休假和辞职，选举问题会变得略微复杂。班长或业务员都可能会突然休假或辞职。

休假则立马放飞自我，不会主动告诉任何人，也不接听电话也不做任何工作交接，好在不会弄丢笔记本，休假结束后继续使用之前笔记本工作。

辞职与休假的区别是，还会额外丢掉笔记本。。。新员工接替是只能新领一个空白笔记本了。。。

基于这个背景，首先需要一个心跳机制，否则业务员很久没听到班长电话，也不知道是因为最近没业务同步还是班长休假了，总之没收到电话一切未知。因此要求班长就算没事也要10分钟心跳，给业务员打电话。若业务员30分钟没收到电话，就知道班长休假或辞职了。一旦某业务页发现班长不见了，为了不影响业务进行，他会发起选举尝试选出新班长，替代之前班长的工作。

**更应该举荐谁为新班长？**

因为任期号只增的，谁的笔记本上最后一页记录的任期号最大，谁的记录就最全，更应该当班长。若存在多个人任期号一致，那看谁的页码号最大，谁当班长合适。如果大家页码数和最后页码的任期号都一致，那无所谓了。每个人投票只能投给记录信息比自己全的或跟自己一样全的人当班长。

**任何人发现班长离职或休假后，都会主动发起选举并举荐自己，将当前任期号+1作为自己想竞选的任期，然后电话问每一个业务员，告知竞选任期和笔记本数据情况，尝试获得选票。获得3票就可以当选新班长。**

如图，业务员1突然离职，若业务员4或业务员5发起投票，都能获取4票当选。业务员2或业务员3发起投票，则最多只能获得2票，当选不了。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261458172.png" alt="image-20250826145826013" style="zoom: 33%;" />

**同时发起投票怎么处理？**

若业务员4和业务员5同时发起投票，为了避免两人同时当选，必须做一些限制，**每个人只能给某一个竞选任期投一次票**。这样避免了同时当选，但是偶尔会战平。因此发起投票之前要随机等待错开，避免多人同时发起竞选。

**休假归来（主节点网络故障恢复）？**

假如此时业务员5当选班长，同时业务员1休假归来。而业务员1的笔记本上还记录了没来得及同步给其他人的信息。。。

这是业务员1不知道有没有新班长，至少他知道自己休假之前是班长，会继续履行班长职责。打电话通知各方更新事件。但是各业务员已经在比老班长任期+1的任期选举了业务员5当班长，自然就拒绝业务员1的请求。同时业务员1收到业务员5新班长的电话后，得知对方已经当选，会自降为业务员，然后按照新班长的指示抹掉未同步的信息，并赶上更新进度。

**电话坏了（从节点网络故障恢复）？**

如果业务员4电话坏了，每30分钟收不到班长电话，就发起选举+1，竞选班长。一段时间后+100了。。。而此时业务员2业余3和班长5正常同步并且任期远没有业务员4高。这是业务员4的电话突然修好了，发起竞选消息打给了班长5，班长5发现任期这么高，不得不自降身份为业务员。但是就算这样4因为之前消息不同，笔记本没有新纪录更新内容老旧，在进行笔记本内容鲜度pk会被刷掉，也不可能当选新班长。其他笔记本信息更全的业务员会当选，所以电话坏了修好后无问题。

总结一下完整流程：

**正常业务流程，班长电话要告知的内容：**

1. 我的任期号
2. 我是哪个支行
3. 前一页的页码、任期号
4. 要写入的页码和事件
5. 确认可以落库的最大号码

**班长对收到回复的反映：**

1. 业务员确认成功：更新该分行记录下来的页码数，更新下一次给他发的页码，当前+1
2. 业务员返回失败：更新下一次给他发的页码数，当前-1
3. 业务员回复任期比自己大，自动降为业务员

**业务员接到电话也做的操作：**

1. 判断班长任期号，如果比自己小，则不认；如果比自己大，则更新自己任期号。
2. 如果某页任期号不同于此次班长同步的，要将改页内容改成当前同步的，并删除后续页内容。
3. 比较班长传来的落库页码，如果比自己页码大。先把这次同步的内容写笔记本，然后比较自己笔记本最大页码和传来落库页码哪个小，按小的开始落库

**业务员返回格式：**

1. 当前任期
2. 成功或是失败

**选举电话的内容：**

1. 我是哪个支行
2. 我要竞选的任期号
3. 我的最后一页页码和任期

**根据选举回复做出的反应：**

1. 若连带自己获得三票，将自己变为班长
2. 若收到回复的任期号大于自己竞选的，将自己变为业务员

**业务员的反映：**

1. 若竞选人任期号比自己小，不支持
2. 竞选人任期号比自己大，删掉自己投票记录后看下信息是否比自己多，看最后一页页码和任期号判断，如果跟自己一样多或更多都要投票，否则不投

**业务员的回复：**

1. 支持或不支持
2. 自己的任期号

**日常规范：**

1. 落库了才算安全记录完一个事件
2. 班长发现最后记录事件页码刚好超过业务员记录的页码，下次要给这家发这一页的内容更新。
3. 班长根据各家记录到那一页信息，结合3家落库规则，来确定落库页码，要电话通知
4. 班长就算没有事也要电话心跳保活
5. 业务员如果3次没收到心跳电话，随机延迟后发起竞选
6. 发起竞选是，任期+1，给自己投一票，然后打电话所有人拉选票。3票当选，若竞选期间收到其他班长消息，通过任期判断谁小谁放弃班长或班长竞选。
7. 竞选人如果5次心跳左右，没选上也没产生新的班长，就重新发起选举。（冲突重试）
8. 任何人确认可落库的页码比实际要大，就赶紧追赶落库。
9. 收到消息任期号比自己大，就更新自己任期号并降为业务员。

**达成效果：**

1. 任意两个支行如果笔记本如果一页相同，那么这一页之前所有内容相同
2. 班长绝不会改动已经写入笔记本的内容
3. 任何时候至多一个班长
4. 已被确认可落库的内容，任何新人竞选上来，也一定有这个记录，不会缺内容。
5. 有3个人确认写在笔记上，并且是该任期记录的，才可以落库。

原因要证明一下，这是这个算法比较难理解的一个细节。我们拿raft算法拉论文中的配图进行解释。

https://raft.github.io/raft.pdf

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261458833.png" alt="image-20250826145846732" style="zoom:33%;" />

S1-S5表示5家分行

(a)-(e)是时间线

方格表示记录的内容，不同颜色表示内容不一样，方格内的数字是任期。

黑线框柱的是当时的班长

(a)时期，S1是班长，他的任期是2，他将黄色内容同步给了S2，然后到达(b)时期

(b)时期，S1休假，S5凭借S3 S4 S5的选票当选新班长，他的任期为3，并记录了蓝色的数据到笔记本

(c)时期，S1休假归来，记录了新红色事件，并且当前任期是4。之后又给S3的第2页填充黄色的，他在第2任期收到的数据。这时问题来了，黄色已经有三家了，要不要发起落库。

先说结论，这时不要落库的。然后我们反证一下，看看落库会怎样。

黄色落库后，来到(d)时期，S5竞争到班长，他会吧之前记录的第二页的蓝色数据刷各家的笔记本，然后笔记本都刷完成蓝色事件。这是也要落库。问题来了(c)阶段第二页落库了黄色事件，如果覆写，会导致不一致。具体来说，其实黄色方块是一条binlog已经在S1-S3执行了，这时候再对S1-S5同时执行一下另外一条蓝色binlog。如果黄色内容是“张三开户存5万”，蓝色内容是“李四开户存10万”结果就是(d)落库后，S1-S3库中有一条“张三开户存5万”的数据，并且无法同步到S4和S5。这违背了数据一致性。所以反推出(c)这一步，黄色写完笔记本后不能落库。

这时我们发现根本原因是班长只能确认笔记本上当前任期的数据安全，不能确认笔记本上之前任期的数据安全。比如黄色就是之前任期的数据，可以看出他跟蓝色冲突，都写在第二页。若贸然确认落库黄色，则蓝色还是有机会当选班长，之后写蓝色数据库就冲突了。

我们假如没有(d)过程直接到(e)过程，(c)中黄色不落库，来到(e)，同步完红色内容后，因为红色内容是当前任期写入的，可以确保安全，这时候就可以同时落库黄色和红色了。这是因为，S5不可能在竞选上来尝试刷第二页蓝色内容了。这是因为S5竞争不过S1-S3，因为其页码更大。

那么蓝色数据怎么办，永远无法落库。但是一段时间后会报写入失败给客户端，明确告诉客户这个操作失败了。这是可以接受的，因为该方案本来就是放弃了**可用性**来换取强一致性。

**可用性定义：非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）**

因此非故障节点，在合理的时间内返回来错误和超时，是能接受的。确保数据一致的。而且可以补救，客户端自己判断蓝色数据重要的话，可以重新发起写入请求。再次出行碰撞冲突的概率是极低的。

至此，raft算法分析完成。修正结论5，有3个人确认写在笔记本，并且是当前任期写入的数据，才可以发起落库。

### 4. 索引

**一级索引** 索引和数据存储在一起，都存储在同一个B+tree中的叶子节点。一般主键索引都是一级索引。

全值匹配、匹配最左前缀。

**二级索引** 二级索引树的叶子节点存储的是主键而不是数据。也就是说，在找到索引后，得到对应的主键，再回到一级索引中找主键对应的数据记录。

**哈希索引** 只有精确匹配索引所有列的查询才有效。查找的速度非常快。

- 哈希索引不是按照索引顺序存储的，无法用于排序。
- 不支持部分索引列匹配查找。
- 不支持范围查找。

### 5. 分布式事务

跨服务数据一致性的解决方案：

**2PC/3PC**：强一致性但存在阻塞问题，适用于金融交易

**TCC模式**：业务补偿型事务，需实现Try-Confirm-Cancel接口

**SAGA模式**：通过事件编排实现最终一致，适用于长事务

**本地消息表**：结合消息队列和异步重试，实现最终一致

### 6. 缓存一致性

#### **旁路缓存模式（Cache Aside）**

**核心逻辑**：
写操作直接更新数据库，并**删除缓存**​（非更新）；读操作先查缓存，未命中时从数据库加载并回填。

**优点**：

天然防缓存穿透（删除操作避免脏数据残留）

实现简单，适合读多写少场景，如商品详情页缓存

**缺点**：

短暂不一致窗口（删除缓存后到下次加载前）

需处理时序问题（如“先删缓存再写DB”与“先写DB再删缓存”的选择）

**适用场景**：对最终一致性容忍度较高的系统（如资讯类应用）。

#### **双写策略**

同步双写（Write-Through）

**核心逻辑**：写入数据库后**同步更新缓存**，保证强一致性

**优点**：数据实时一致，适合金融交易等高要求场景

**缺点**：

- 写入延迟高（需等待缓存和DB同时完成）
- 并发写可能导致脏数据覆盖（如两个线程同时更新同一缓存）

异步双写（Write-Behind）

**核心逻辑**：先更新缓存，数据库更新**异步批量执行**

**优点**：写入性能高，适合秒杀等高并发场景

**缺点**：

存在短暂数据不一致风险（异步延迟期间）

需处理异步任务失败补偿（如通过消息队列重试）

#### **延迟双删策略**

**核心逻辑**：

首次删除缓存 → 更新数据库 → 延迟一定时间 → 再次删除缓存

**优点**：

缓解并发读写导致的脏数据问题（如读请求在更新DB后回填旧缓存）

兼容主从同步延迟场景（如MySQL主从复制）

**难点**：

延迟时间需根据业务评估（通常略大于读请求处理时间）

需维护延迟任务队列，可能引入复杂度

#### **消息队列 + Binlog订阅**

**核心逻辑**：

业务代码仅操作数据库，通过**监听Binlog**（如Canal、Debezium）捕获数据变更。

将变更事件推送至消息队列（如Kafka），由消费者异步更新缓存**235**。

**优点**：

业务解耦，避免代码侵入

严格保证操作顺序性（基于Binlog日志顺序）

**缺点**：

架构复杂度高（需部署中间件如Canal）

同步延迟不可控（取决于Binlog解析和消息处理速度）

#### **分布式事务方案**

**核心逻辑**：通过**2PC（两阶段提交）**或**TCC（Try-Confirm-Cancel）**保证缓存与数据库的原子性操作**13**。

**优点**：强一致性保障，适合支付、库存扣减等关键链路**36**。

**缺点**：

性能损耗显著（同步阻塞）

事务回滚逻辑复杂（需设计补偿机制）

#### **版本控制与乐观锁**

**核心逻辑**：

为数据添加版本号或时间戳，更新时校验版本是否匹配

若冲突则重试或丢弃（如Redis的WATCH命令）

**优点**：无锁设计，适合低冲突场景（如评论点赞）

**缺点**：需业务层处理重试逻辑，可能增加代码复杂度

#### 选型建议

| **场景特征**                 | **推荐方案**          | **一致性等级** | **性能影响** |
| :--------------------------- | :-------------------- | :------------- | :----------- |
| 读多写少，容忍短暂延迟       | Cache Aside           | 最终一致       | 低           |
| 高并发写入（如秒杀）         | 异步双写 + 消息队列   | 最终一致       | 中           |
| 金融交易类强一致性要求       | 分布式事务（2PC/TCC） | 强一致         | 高           |
| 多服务协作复杂系统           | Binlog订阅 + 消息队列 | 最终一致       | 中           |
| 低冲突并发（如社交动态更新） | 乐观锁 + 版本控制     | 最终一致       | 低           |

**补充优化策略**

1. **缓存预热**：启动时预加载热点数据，减少冷启动期缓存穿透**。**
2. **布隆过滤器**：拦截无效请求，防止缓存穿透（如查询不存在的ID）**。**
3. **熔断降级**：缓存失效时返回默认值，避免数据库过载**。**

### 7. 布隆过滤器

布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是**空间效率**和**查询时间**都**远远超过一般的算法**，缺点是有一定的误识别率和删除困难。

布隆过滤器的原理：当一个元素被加入集合时，通过 K 个散列函数将这个元素映射成一个位数组中的 K 个点，把它们置为 1。检索时，我们只要看看这些点是不是都是 1 就（大约）知道集合中有没有它了：如果这**些点有任何一个 0**，则**被检元素一定不在**；如果**都是 1**，则被检元素**很可能在**。

简单来说就是准备一个长度为 m 的位数组并初始化所有元素为 0，用 k 个散列函数对元素进行 k 次散列运算跟 len (m) 取余得到 k 个位置并将 m 中对应位置设置为 1。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261459695.png" alt="image-20250826145908630" style="zoom:33%;" />

布隆过滤器包含如下四个属性：

- k : 哈希函数个数
- m : 位数组长度
- n : 插入的元素个数
- p : 误判率

若位数组长度太小则会导致所有 bit 位很快都会被置为 1 ，那么检索任意值都会返回”可能存在“ ， 起不到过滤的效果。 位数组长度越大，则误判率越小。

同时，哈希函数的个数也需要考量，哈希函数的个数越大，检索的速度会越慢，误判率也越小，反之，则误判率越高。

<img src="https://raw.githubusercontent.com/arminxu-coding/image/main/2025/202508261459184.png" alt="image-20250826145923086" style="zoom:33%;" />

数据上云应用：[https://iwiki.woa.com/p/4013238045](https://iwiki.woa.com/p/4013238045#四、接口和缓存系统设计（兼容DCache协议）)